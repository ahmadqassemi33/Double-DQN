import numpy as np
import gym
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch as T

import collections
import cv2

import os

class DoubleDeepQNetwork(nn.Module):
  def __init__(self, lr, input_dims, name, n_actions, chkpt_dir):
    super(DoubleDeepQNetwork, self).__init__()
    self.checkpoint_dir = chkpt_dir
    self.checkpoint_file = os.path.join(self.checkpoint_dir, name)

    self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)
    self.conv2 = nn.Conv2d(32,64,4,stride=2)
    self.conv3 = nn.Conv2d(64,64,3,stride=1)

    fc_input_dims = self.calculate_conv_output_dims(input_dims)
    
    self.fc1 = nn.Linear(fc_input_dims,512)
    self.fc2 = nn.Linear(512, n_actions)

    self.optimizer = optim.RMSprop(self.parameters(), lr = lr)
    self.loss = nn.MSELoss()
    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
    self.to(self.device)

  def calculate_conv_output_dims(self, input_dims):
    state = T.zeros(1,*input_dims)
    dims = self.conv1(state)
    dims = self.conv2(dims)
    dims = self.conv3(dims)
    return int(np.prod(dims.size()))

  def forward(self, state):
    conv1 = F.relu(self.conv1(state))
    conv2 = F.relu(self.conv2(conv1))
    conv3 = F.relu(self.conv3(conv2))
    conv_state = conv3.view(conv3.size()[0], -1)
    flat1 = F.relu(self.fc1(conv_state))
    actions = self.fc2(flat1)
    return actions

  def save_checkpoint(self):
    print('...saving checkpoint...')
    T.save(self.state_dict(), self.checkpoint_file)

  def load_checkpoint(self):
    print('...checkpoint loading')
    self.load_state_dict(T.load(self.checkpoint_file))
def plot_learning_curve(x, scores, eps_history, filename):
  fig = plt.figure()
  ax = fig.add_subplot(111, label='1')
  ax2 = fig.add_subplot(111, label='2', frame_on=False)

  ax.plot(x, eps_history, color="C0")
  ax.set_xlabel("Training steps", color="C0")
  ax.set_ylabel("Epsilon", color="C0")
  ax.tick_params(axis='x', colors="C0")
  ax.tick_params(axis='y', colors="C0")

  N = len(scores)
  running_ave = np.empty(N)
  for t in range(N):
    running_ave[t] = np.mean(scores[max(0, t-20):(t+1)])

  ax2.scatter(x, running_ave, color="C1")
  ax2.axes.get_xaxis().set_visible(False)
  ax2.yaxis.tick_right()
  ax2.set_ylabel('Score', color="C1")
  ax2.yaxis.set_label_position('right')
  ax2.tick_params(axis='y', colors="C1")

  if lines is not None:
    for line in lines:
      plt.axvline(x=line)

  plt.savefig(filename)

class RepeatActionAndMaxFrame(gym.Wrapper):
  def __init__(self, env=None, repeat = 4):
    super(RepeatActionAndMaxFrame, self).__init__(env)
    self.repeat = repeat
    self.shape = env.observation_space.low.shape
    self.frame_buffer = np.zeros_like((2, self.shape))

  def step(self, action):
    t_reward = 0.0
    done = False
    for i in range(self.repeat):
      obs, reward, done, info = self.env.step(action)
      t_reward += reward
      idx = i % 2
      self.frame_buffer[idx] = obs

      if done:
        break

    max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])
    return max_frame, t_reward, done, info

  def reset(self):
    obs = self.env.reset()
    self.frame_buffer = np.zeros_like((2, self.shape))
    self.frame_buffer[0] = obs

    return obs

class PreprocessFrame(gym.ObservationWrapper):
  def __init__(self, shape, env=None):
    super(PreprocessFrame, self).__init__(env)
    self.shape = (shape[2], shape[0], shape[1])
    self.observation_space = gym.spaces.Box(low=0.0,high=1.0,shape=self.shape,
                                            dtype=np.float32)
    
  def observation(self, obs):
    new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
    resized_screen = cv2.resize(new_frame, self.shape[1:],
                                interpolation=cv2.INTER_AREA)
    new_obs = np.array(resized_screen, dtype=np.bool).reshape(self.shape)
    new_obs = new_obs / 255.0

    return new_obs

class StackFrames(gym.ObservationWrapper):
  def __init__(self, env, repeat):
    super(StackFrames, self).__init__(env)
    self.observation_space = gym.spaces.Box(
                        env.observation_space.low.repeat(repeat, axis=0),
                        env.observation_space.high.repeat(repeat, axis=0),
                        dtype=np.float32)
    self.stack = collections.deque(maxlen=repeat)

  def reset(self):
    self.stack.clear()
    observation = self.env.reset()
    for _ in range(self.stack.maxlen):
      self.stack.append(observation)

    return np.array(self.stack).reshape(self.observation_space.low.shape)

  def observation(self, observation):
    self.stack.append(observation)
    return np.array(self.stack).reshape(self.observation_space.low.shape)  

def make_env(env_name, shape = (84,84,1), repeat=4):
  env = gym.make(env_name)
  env = RepeatActionAndMaxFrame(env, repeat)
  env = PreprocessFrame(shape, env)
  env = StackFrames(env, repeat)

  return env  
  
class ReplayBuffer():
  def __init__(self, mem_size, input_dims, n_action):
    self.mem_cntr = 0
    self.mem_size = mem_size
    self.input_dims = input_dims
    self.mem_state = np.zeros((self.mem_size, *input_dims), dtype=np.float32)
    self.mem_state_ = np.zeros((self.mem_size, *input_dims), dtype=np.float32)
    self.mem_reward = np.zeros(self.mem_size, dtype = np.float32)
    self.mem_action = np.zeros(self.mem_size, dtype = np.int64)
    self.mem_terminal = np.zeros(self.mem_size, dtype = np.bool)

  def store_transition(self, state, action, reward, state_, done):
    index = self.mem_cntr % self.mem_size
    self.mem_state[index] = state
    self.mem_state_[index] = state_
    self.mem_reward[index] = reward
    self.mem_terminal[index] = done
    self.mem_action[index] = action
    self.mem_cntr += 1

  def sample_buffer(self, batch_size):
    max_position = min(self.mem_cntr, self.mem_size)
    index = np.random.choice(max_position, batch_size, replace=False)
    state = self.mem_state[index]
    state_=self.mem_state_[index]
    action = self.mem_action[index]
    reward = self.mem_reward[index]
    done = self.mem_terminal[index]
    return state, action, reward, state_, done
    
class DDQNAgent():
  def __init__(self, lr, gamma, epsilon, n_action, mem_size, input_dims, batch_size, 
               replace=1000, eps_dec=5e-7, eps_min=0.01, 
               env_name=None, algo=None, chkpt_dir='tmp/ddqn'):
    self.lr = lr
    self.gamma = gamma
    self.n_action = n_action
    self.input_dims = input_dims
    self.env_name = env_name
    self.algo = algo
    self.chkpt_dir = chkpt_dir
    self.replace_target_cnt = replace
    self.epsilon = epsilon
    self.eps_min = eps_min
    self.eps_dec=eps_dec
    self.learn_step_counter = 0 
    self.batch_size = batch_size
    self.action_space = [i for i in range(self.n_action)]
    
    self.memory = ReplayBuffer(mem_size, input_dims, n_action)
    
    self.q_eval = DoubleDeepQNetwork(self.lr, input_dims=self.input_dims, 
                               n_actions=self.n_action, 
                               name=self.env_name+'_'+self.algo+'_q_eval', 
                               chkpt_dir=self.chkpt_dir)
    self.q_next = DoubleDeepQNetwork(self.lr, input_dims=self.input_dims, 
                               n_actions=self.n_action, 
                               name=self.env_name+'_'+self.algo+'_q_next', 
                               chkpt_dir=self.chkpt_dir)
    
  def choose_action(self, observations):
    if np.random.random() > self.epsilon:
      state = T.tensor([observations], dtype=T.float).to(self.q_eval.device)
      actions = self.q_eval.forward(state)
      action = T.argmax(actions).item()
    else:
      action = np.random.choice(self.action_space)
    return action

  def store_transition(self, state, action, reward, state_, done):
    self.memory.store_transition(state, action, reward, state_, done)

  def sample_memory(self):
    state, action, reward, state_, done = self.memory.sample_buffer(self.batch_size)
    states = T.tensor(state).to(self.q_eval.device)
    actions = T.tensor(action).to(self.q_eval.device)
    rewards = T.tensor(reward).to(self.q_eval.device)
    states_ = T.tensor(state_).to(self.q_eval.device)
    dones = T.tensor(done).to(self.q_eval.device)
    return states, actions, rewards, states_, dones

  def replace_target_network(self):
    if self.learn_step_counter % self.replace_target_cnt == 0:
      self.q_next.load_state_dict(self.q_eval.state_dict())

  def decrement_epsilon(self):
    self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min

  def save_models(self):
    self.q_eval.save_checkpoint()
    self.q_next.save_checkpoint()

  def load_models(self):
    self.q_eval.load_checkpoint()
    self.q_next.load_checkpoint() 

  def learn(self):
    if self.memory.mem_cntr < self.batch_size:
      return

    self.q_eval.optimizer.zero_grad()
    
    self.replace_target_network()

    states, actions, rewards, states_, dones = self.sample_memory()

    indices = np.arange(self.batch_size)

    q_pred = self.q_eval.forward(states)[indices, actions] # without indices, dims --> batch_size*n_actions
    q_next = self.q_next.forward(states_)
    q_eval = self.q_eval.forward(states_)

    max_actions = T.argmax(q_eval, dim=1)
    #q_next = self.q_next.forward(states_).max(dim=1)[0]

    q_next[dones] = 0.0
    q_target = rewards + self.gamma*q_next[indices, max_actions]

    loss = self.q_eval.loss(q_target,q_pred).to(self.q_eval.device)
    loss.backward()
    self.q_eval.optimizer.step()
    self.learn_step_counter += 1 

    self.decrement_epsilon()

if __name__ == '__main__':
  env = make_env('PongNoFrameskip-v4')
  eps_history, scores, steps_array = [], [], [] # steps_array works as x in the horizontal axis
  best_score = -np.inf
  n_games = 500
  load_checkpoint = False
  agent = DDQNAgent(lr=0.0001, gamma=0.99, n_action=env.action_space.n, 
                   mem_size=20000, input_dims=(env.observation_space.shape), 
                   epsilon=1.0, eps_min = 0.1, batch_size=32, 
                   eps_dec=1e-5, replace=1000, chkpt_dir='models/', 
                   algo='DDQNAgent', env_name='PongNoFrameskip-v4')
  
  if load_checkpoint:
    agent.load_models()

  fname = agent.algo +'_'+agent.env_name + '_lr' + str(agent.lr) + '_' + \
          '_' + str(n_games) + 'games'
  figure_file = 'plots/' + fname + '.png'

  n_steps = 0

  for i in range(n_games):
    done = False
    observation = env.reset()
    score = 0

    while not done:
      action = agent.choose_action(observation)
      observation_, reward, done, info = env.step(action)
      score += reward

      if not load_checkpoint:
        agent.store_transition(observation, action, reward, observation_, int(done))
        agent.learn()

      observation = observation_
      n_steps += 1

    scores.append(score)
    steps_array.append(n_steps)
    eps_history.append(agent.epsilon)

    if i % 100 == 0:
      ave_score = np.mean(scores[-100:])
      print('episode=', i, 'score=', score, 
            'aver_score %0.1f best score %0.1f epsilon %0.2f ' 
            %(ave_score, best_score, agent.epsilon), 'step=', n_steps)

    if ave_score > best_score:
      if not load_checkpoint:
        agent.save_models()
      best_score = ave_score

plot_learning_curve(steps_array, scores, eps_history, figure_file)
